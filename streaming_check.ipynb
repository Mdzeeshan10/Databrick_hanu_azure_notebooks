{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc2dd63-5965-4a52-96dd-4679e8a6fd5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "IOT_CS = \"Endpoint=sb://ihsuprodpnres019dednamespace.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=FS7yH98azfRbvfhHW/q+nrsTTTSz/42pnescbtJoVFU=;EntityPath=iothub-ehub-iothubchec-24758816-65b2d26c78\" # dbutils.secrets.get('iot','iothub-cs') # IoT Hub connection string (Event Hub Compatible)\n",
    "ehConf = { \n",
    "  'eventhubs.connectionString':sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(IOT_CS),\n",
    "  'ehName':\"iothub-ehub-iothubchec-24758816-65b2d26c78\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2000d3-9ff9-4ea1-915d-cc6a19fab24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"fs.azure.account.key.zee.dfs.core.windows.net\", \"YlBjJI7gIJpTUQ5ELXbqiqI/T/WhLRDi03zTBEiVU30O0O9zXwkWqKULPst5k+5cors7NYTbyw/v+AStJ3bIBw==\")\n",
    "\n",
    "# Setup storage locations for all data\n",
    "ROOT_PATH = f\"abfss://iot@zee.dfs.core.windows.net/\"\n",
    "BRONZE_PATH = ROOT_PATH + \"bronze/\"\n",
    "SILVER_PATH = ROOT_PATH + \"silver/\"\n",
    "GOLD_PATH = ROOT_PATH + \"gold/\"\n",
    "SYNAPSE_PATH = ROOT_PATH + \"synapse/\"\n",
    "CHECKPOINT_PATH = ROOT_PATH + \"checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339522f5-1131-477e-ac9e-6a56e67e4b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Schema of incoming data from IoT hub\n",
    "schema = \"timestamp timestamp, deviceId string, temperature double, humidity double, windspeed double, winddirection string, rpm double, angle double\"\n",
    "\n",
    "\n",
    "# Read directly from IoT Hubs using the EventHubs library for Azure Databricks\n",
    "iot_stream = (\n",
    "    spark.readStream.format(\"eventhubs\")                                        # Read from IoT Hubs directly\n",
    "    .options(**ehConf)                                                        # Use the Event-Hub-enabled connect string\n",
    "    .load()                                                                   # Load the data\n",
    "    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema)) # Extract the payload from the messages\n",
    "    .select('reading.*', F.to_date('reading.timestamp').alias('date'))        # Create a \"date\" field for partitioning\n",
    ")\n",
    "\n",
    "# Split our IoT Hubs stream into separate streams and write them both into their own Delta locations\n",
    "write_turbine_to_delta = (\n",
    "    iot_stream.filter('temperature is null')                          # Filter out turbine telemetry from other streams\n",
    "    .select('date','timestamp','deviceId','rpm','angle')            # Extract the fields of interest\n",
    "    .writeStream.format('delta')                                    # Write our stream to the Delta format\n",
    "    .partitionBy('date')                                            # Partition our data by Date for performance\n",
    "    .option(\"checkpointLocation\", ROOT_PATH + \"/bronze/cp/turbine\") # Checkpoint so we can restart streams gracefully\n",
    "    .start(ROOT_PATH + \"/bronze/data/turbine_raw\")                  # Stream the data into an ADLS Path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e66efebe-c55a-487c-9194-d0227c1fb4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0344174-d94e-4ea8-a01e-5aa6e5175b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(iot_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0c6a75-d27d-45e9-b380-77c347e0d982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82e5ff4-dbe3-4ad2-82d0-21fba59320d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create functions to merge turbine and weather data into their target Delta tables\n",
    "def merge_records(incremental, target_path): \n",
    "    incremental.createOrReplaceTempView(\"incremental\")\n",
    "    \n",
    "# MERGE consists of a target table, a source table (incremental),\n",
    "# a join key to identify matches (deviceid, time_interval), and operations to perform \n",
    "# (UPDATE, INSERT, DELETE) when a match occurs or not\n",
    "    incremental._jdf.sparkSession().sql(f\"\"\"\n",
    "        MERGE INTO turbine_hourly t\n",
    "        USING incremental i\n",
    "        ON i.date=t.date AND i.deviceId = t.deviceid AND i.time_interval = t.time_interval\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Perform the streaming merge into our  data stream\n",
    "turbine_stream = (\n",
    "    spark.readStream.format('delta').table('turbine_raw')        # Read data as a stream from our source Delta table\n",
    "    .groupBy('deviceId','date',F.window('timestamp','1 hour')) # Aggregate readings to hourly intervals\n",
    "    .agg({\"rpm\":\"avg\",\"angle\":\"avg\"})\n",
    "    .writeStream                                                                                         \n",
    "    .foreachBatch(merge_records)                              # Pass each micro-batch to a function\n",
    "    .outputMode(\"update\")                                     # Merge works with update mod\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebcc19df-609a-47fa-87f3-a17845abd02d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052655fb-d6f7-47ff-95db-cb84e902a439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871c1a86-f0c5-4f58-96ee-044c925da87d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "iot_stream = (\n",
    "    spark.readStream.format(\"eventhubs\")                                        # Read from IoT Hubs directly\n",
    "    .options(**ehConf)                                                        # Use the Event-Hub-enabled connect string\n",
    "    .load()                                                                   # Load the data\n",
    "    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema)) # Extract the payload from the messages\n",
    "    .select('reading.*', F.to_date('reading.timestamp').alias('date'))        # Create a \"date\" field for partitioning\n",
    ")\n",
    "\n",
    "\n",
    "(iot_stream.writeStream\n",
    "   .format(\"delta\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"checkpointLocation\", \"/tmp/delta/_checkpoints/\")\n",
    "   .start(\"/delta/events\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3356a23b-05b7-432a-9bd1-c4b1d8c43428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "IOT_CS1 = \"Endpoint=sb://iothub-ns-iotsource-24760747-aece8adb55.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=avxtSdCezB2uw9o4cFlkg7X3pBNDxV8YHTmdHLWF1OU=;EntityPath=iotsource\" # dbutils.secrets.get('iot','iothub-cs') # IoT Hub connection string (Event Hub Compatible)\n",
    "ehConf = { \n",
    "  'eventhubs.connectionString':sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(IOT_CS1),\n",
    "  'ehName':\"iotsource\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c82b1ea4-3072-427a-a4f5-e55e3de48584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ffc64b5-0663-4805-b906-80797db4e326",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e5d2ab-97a9-43c2-bf33-6d91eabb1218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import *\n",
    "\n",
    "# deltaTable = DeltaTable.forPath(spark, \"/data/mydata\")\n",
    "\n",
    "\n",
    "\n",
    "iot_stream1 = (\n",
    "    spark.readStream.format(\"eventhubs\")                                        # Read from IoT Hubs directly\n",
    "    .options(**ehConf)                                                        # Use the Event-Hub-enabled connect string\n",
    "    .load()                                                                   # Load the data\n",
    "    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema)) # Extract the payload from the messages\n",
    "    .select('reading.*', F.to_date('reading.timestamp').alias('date'))        # Create a \"date\" field for partitioning\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "(iot_stream1.writeStream\n",
    "   .format(\"delta\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"checkpointLocation\", \"dbfs:/user/hive/warehouse/zee/\")\n",
    "   .toTable(\"events1\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bc789f-e753-4744-98f0-d005f219730f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(iot_stream)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 896053401196694,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_check",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
